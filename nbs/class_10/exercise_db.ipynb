{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class 10 - Image encoding and classification using CNNs and ViTs\n",
    "In this class, we will use pretrained CNN and transformer models to encode and classify images.\n",
    "We will use a library you are familiar with, namely Hugging Face's _transformers_, as well as (under the hood) _timm_ (another HuggingFace library, specific to computer vision) and _pytorch_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions\n",
    "**Overall goal**: training a simple fully-connected neural network that classifies snack types from this dataset (https://huggingface.co/datasets/Matthijs/snacks) based on image embeddings extracted from a pretrained ResNet model (https://arxiv.org/abs/1512.03385)\n",
    "\n",
    "**Suggestion**: use `example.ipynb` notebook if you are in doubt! This might be especially helpful with respect to Pytorch. All we will use Pytorch for, here, are things we already did as part of NLP, but if you have not used Pytorch since it is perfectly understandable that you need a reminder.\n",
    "\n",
    "Steps:\n",
    "1. Use Hugging Face's `dataset` library to import the dataset: https://huggingface.co/datasets/Matthijs/snacks. Import the training and the test splits from this dataset, and visualize the frequency of each class.\n",
    "2. Load a pretrained ResNet model using HuggingFace's `transformers`. Check the documentation (https://huggingface.co/docs/transformers/en/model_doc/resnet, https://huggingface.co/microsoft/resnet-50) for more details on how to load the model. Visualize the structure of the model, and reflect on how each of the blocks works, based on what we discussed in the lecture;\n",
    "3. Extract ResNet embeddings for all the images in the training set and all the images in the test set. You will need to look for the `pooler_output` attribute of the model's output. This sounds complex, but it's really just a minor change over the examples presented in the model's documentation;\n",
    "4. Optional: apply dimensionality reduction to the embeddings using TSNE (https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html): is any information about the labels encoded in the embeddings?\n",
    "5. Train a simple neural network that, for each image in the training set, takes the embeddings as inputs and predicts the snack label\n",
    "    - You will have to create a Pytorch `Dataset` (https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) where both your embeddings and your labels are included.\n",
    "    - You will also have to create a `Dataloader` for the training set, as this will make it much easier to do things like batching and shuffling\n",
    "    - You need to specify the loss and the optimizer\n",
    "    - Then, you need to specify a training loop. Remember the neural net gymnastics:\n",
    "        - Do the forward step, i.e., make a prediction\n",
    "        - Compute the loss based on predictions and true labels \n",
    "        - Compute gradients\n",
    "        - Propagate gradients (backpropagation)\n",
    "6. Evaluate the resulting model:\n",
    "    - Make predictions for all examples in the test set\n",
    "    - Use `sklearn.metrics` utils (https://scikit-learn.org/stable/modules/model_evaluation.html#classification-metrics) to compute accuracy and F1-score\n",
    "    - Also plot the confusion matrix, to see which categories are most often confused\n",
    "7. Explore zero-shot classification for computer vision\n",
    "    - Try classification on custom labels on one of the images from today's dataset. To do so, adapt the code provided here: https://huggingface.co/docs/transformers/en/tasks/zero_shot_image_classification, by simply replacing the desired labels and the input\n",
    "    - Can you get reasonable predictions on more nuanced labels than the ones provided with the dataset?\n",
    "    - Which kind of models support this behavior? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label dictionary: inferred from the dataset's online documentation\n",
    "label_dict = {0: 'apple',\n",
    "              1: 'banana',\n",
    "              2: 'cake',\n",
    "              3: 'candy',\n",
    "              4: 'carrot',\n",
    "              5: 'cookie',\n",
    "              6: 'doughnut',\n",
    "              7: 'grape',\n",
    "              8: 'hotdog',\n",
    "              9: 'icecream',\n",
    "              10: 'juice',\n",
    "              11: 'muffin',\n",
    "              12: 'orange',\n",
    "              13: 'pineapple',\n",
    "              14: 'popcorn',\n",
    "              15: 'pretzel',\n",
    "              16: 'salad',\n",
    "              17: 'strawberry',\n",
    "              18: 'waffle',\n",
    "              19: 'watermelon'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/DataSci-AU-24-fork/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 76.5M/76.5M [00:01<00:00, 66.1MB/s]\n",
      "Downloading data: 100%|██████████| 14.9M/14.9M [00:00<00:00, 32.7MB/s]\n",
      "Downloading data: 100%|██████████| 15.2M/15.2M [00:00<00:00, 30.3MB/s]\n",
      "Generating train split: 100%|██████████| 4838/4838 [00:00<00:00, 24284.46 examples/s]\n",
      "Generating test split: 100%|██████████| 952/952 [00:00<00:00, 24698.01 examples/s]\n",
      "Generating validation split: 100%|██████████| 955/955 [00:00<00:00, 25579.27 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"Matthijs/snacks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset['train']\n",
    "test = dataset['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoImageProcessor, ResNetModel\n",
    "import torch\n",
    "from torchview import draw_graph\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"microsoft/resnet-50\")\n",
    "model = ResNetModel.from_pretrained(\"microsoft/resnet-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#terminal: sudo apt-get install graphviz\n",
    "#draw_graph(model, input_size=(1,3,224,224), expand_nested=True).visual_graph "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### task 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _extract(dataset, outs):\n",
    "    for i, image in enumerate(dataset['image']):\n",
    "        inputs = image_processor(image, return_tensors=\"pt\")\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        embedding = outputs.pooler_output\n",
    "        outs.append(embedding.squeeze())\n",
    "    return torch.stack(tuple(outs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_embeddings = []\n",
    "test_embeddings = []\n",
    "\n",
    "train_embeddings = _extract(train, train_embeddings)\n",
    "test_embeddings = _extract(test, test_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### task 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(Dataset):\n",
    "    def __init__(self, labels, embeddings):\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        self.embedding = embeddings\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = self.embedding[idx]\n",
    "        label = self.labels[idx]\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(Embeddings(embeddings=train_embeddings, \n",
    "                                         labels=train['label']),\n",
    "                              batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### task 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(2048, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 128)\n",
    "        self.fc3 = nn.Linear(128, 20)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Classifer()\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus CV tasks\n",
    "- Try training a classifier on embeddings extracted from a vision transformer: how does performance compare to ResNet?\n",
    "- Build a CNN from scratch for this classification task: how does performance compare? There are lots of tutorials online, you can start from this one: https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html.\n",
    "- Look up models and pipelines for more computer vision tasks, including semantic segmentation, object detection (which can also be performed zero-shot using ViT), etc.\n",
    "\n",
    "#### Bonus NN-related tasks\n",
    "- Train NN-based models (FFNNs or RNNs) on the bike data: how does performance compare to the best algorithms?\n",
    "\n",
    "### Additional tasks\n",
    "- A wide array of pretrained models for audio classification (including zero-shot audio classification) are also available on Hugging Face. Experiment with those :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional notes\n",
    "- Here, we worked with pretrained models. If you want to train a CNN from scratch (e.g., because you are working with \"special\" image data, and no pretrained models are available), you can look up, for example, _Pytorch_'s intro tutorial (https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html). There are really a billion resources online if this is what you decide to focus on for your final project;\n",
    "- If, instead, fine-tuning a full model is what you want to do, plenty of tutorials are available for tasks ranging from image classification, object detection, image segmentation, etc. Look up _timm_'s documentation (https://huggingface.co/docs/hub/en/timm) and _transformers_'s documentation (https://huggingface.co/docs/transformers/tasks/image_classification)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
